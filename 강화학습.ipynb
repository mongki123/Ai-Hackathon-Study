{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687261125699,"user":{"displayName":"최지수","userId":"07522837684830544974"},"user_tz":-540},"id":"CJkdZn001ULm"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1687261137984,"user":{"displayName":"최지수","userId":"07522837684830544974"},"user_tz":-540},"id":"8Mh_H-0Q1W1v"},"outputs":[],"source":["class Agent:\n","    def __init__(self):\n","        self.epsilon = 0.1\n","        self.alpha = 0.5\n","        self.state_history = []\n","\n","    def initialize_V(self, env, state_winner_triples):\n","        V = np.zeros(env.max_states)\n","        for state, winner, ended in state_winner_triples:\n","            if ended:\n","                if winner == env.x:\n","                    state_value = 1\n","                else:\n","                    state_value = 0\n","            else:\n","                state_value = 0.5\n","\n","            V[state] = state_value\n","        self.V = V\n","\n","    def set_symbol(self, symbol):\n","        self.symbol = symbol\n","\n","    def reset_history(self):\n","        self.state_history = []\n","\n","    def choose_random_action(self, env):\n","        empty_moves = env.get_empty_moves()\n","        random_index_from_empty_moves = np.random.choice(len(empty_moves))\n","        next_random_move = empty_moves[random_index_from_empty_moves]\n","        return next_random_move\n","\n","    def choose_best_action_from_states(self, env):\n","        next_best_move, best_state = env.get_next_best_move(self)\n","        return next_best_move, best_state\n","\n","    def get_next_move(self, env):\n","        next_best_move, best_state = None, None\n","        random_number = np.random.rand()\n","        if random_number < self.epsilon:\n","            next_best_move = self.choose_random_action(env)\n","        else:\n","            next_best_move, best_state = self.choose_best_action_from_states(env)\n","        return next_best_move, best_state\n","\n","    def take_action(self, env):\n","        selected_next_move, best_state = self.get_next_move(env)\n","        env.board[selected_next_move[0], selected_next_move[1]] = self.symbol\n","\n","    def update_state_history(self, state):\n","        self.state_history.append(state)\n","\n","    def update(self, env):\n","        reward = env.reward(self.symbol)\n","        target = reward\n","        for prev in reversed(self.state_history):\n","            value = self.V[prev] + self.alpha * (target - self.V[prev])\n","            self.V[prev] = value\n","            target = value\n","        self.reset_history()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":342,"status":"ok","timestamp":1687261215553,"user":{"displayName":"최지수","userId":"07522837684830544974"},"user_tz":-540},"id":"CjPZ78wr1o96"},"outputs":[],"source":["class Environment:\n","\n","    def __init__(self):\n","        self.board = np.zeros((7, 7))\n","        self.x = -1\n","        self.o = 1\n","        self.winner = None\n","        self.ended = False\n","        self.max_states = 7 ** (7 * 7)\n","\n","    def is_empty(self, i, j):\n","        return self.board[i, j] == 0\n","\n","    def reward(self, symbol):\n","        collected_reward = 0\n","        if self.game_over() and self.winner == symbol:\n","            collected_reward = 1\n","        return collected_reward\n","\n","    def is_draw(self):\n","        is_draw = False\n","        if self.ended and self.winner is None:\n","            is_draw = True\n","        return is_draw\n","\n","    def get_state(self):\n","        state = 0\n","        loop_index = 0\n","        for i in range(7):\n","            for j in range(7):\n","                if self.board[i, j] == self.x:\n","                    state_value = 1\n","                elif self.board[i, j] == self.o:\n","                    state_value = 2\n","                else:\n","                    state_value = 0  # empty\n","\n","                state += (7 ** loop_index) * state_value\n","                loop_index += 1\n","        return state\n","\n","    def game_over(self):\n","        if self.ended:\n","            return True\n","        players = [self.x, self.o]\n","\n","        for i in range(7):\n","            for player in players:\n","                if self.board[i].sum() == player * 7:\n","                    self.winner = player\n","                    self.ended = True\n","                    return True\n","\n","        for j in range(7):\n","            for player in players:\n","                if self.board[:, j].sum() == player * 7:\n","                    self.winner = player\n","                    self.ended = True\n","                    return True\n","\n","        for player in players:\n","            if self.board.trace() == player * 7:\n","                self.winner = player\n","                self.ended = True\n","                return True\n","\n","            if np.fliplr(self.board).trace() == player * 7:\n","                self.winner = player\n","                self.ended = True\n","                return True\n","\n","        board_with_true_false = self.board == 0\n","        if np.all(board_with_true_false == False):\n","            self.winner = None\n","            self.ended = True\n","            return True\n","\n","        self.winner = None\n","        return False\n","\n","    def get_empty_moves(self):\n","        empty_moves = []\n","        for i in range(7):\n","            for j in range(7):\n","                if self.is_empty(i, j):\n","                    empty_moves.append((i, j))\n","        return empty_moves\n","\n","    def get_next_best_move(self, agent):\n","        best_value = -1\n","        next_best_move = None\n","        best_state = None\n","        for i in range(7):\n","            for j in range(7):\n","                if self.is_empty(i, j):\n","                    self.board[i, j] = agent.symbol\n","                    state = self.get_state()\n","                    self.board[i, j] = 0\n","                    if agent.V[state] > best_value:\n","                        best_value = agent.V[state]\n","                        best_state = state\n","                        next_best_move = (i, j)\n","\n","        return next_best_move, best_state\n","\n","    def draw_board(self):\n","        def __print(to_print, j):\n","            if j == 0:\n","                print(f\"|  {to_print}  \", end=\"|\")\n","            else:\n","                print(f\"{to_print}  \", end=\"|\")\n","\n","        for i in range(7):\n","            print(\" -----------------------------------------------\")\n","            for j in range(7):\n","                print(\"  \", end=\"\")\n","                if self.board[i, j] == self.x:\n","                    __print('x', j)\n","                elif self.board[i, j] == self.o:\n","                    __print('o', j)\n","                else:\n","                    __print(' ', j)\n","            print(\"\")\n","        print(\" -----------------------------------------------\")\n","        print(\"\\n\")\n","\n","\n","class Human:\n","\n","    def set_symbol(self, symbol):\n","        self.symbol = symbol\n","\n","    def take_action(self, env):\n","        while True:\n","            try:\n","                move = input(\"Enter box location to make your move in format of i,j : \")\n","                i, j = [int(item.strip()) for item in move.split(',')]\n","                if env.is_empty(i, j):\n","                    env.board[i, j] = self.symbol\n","                    break\n","                else:\n","                    print(\"Please enter valid move\")\n","            except:\n","                print(\"Please enter valid move\")\n","\n","\n","def get_state_hash_and_winner(env, i=0, j=0):\n","    results = []\n","    for v in [0, env.x, env.o]:\n","        env.board[i, j] = v\n","        if j == 2:\n","            if i == 2:\n","                state = env.get_state()\n","                ended = env.game_over()\n","                winner = env.winner\n","                results.append((state, winner, ended))\n","            else:\n","                results += get_state_hash_and_winner(env, i + 1, 0)\n","        else:\n","            results += get_state_hash_and_winner(env, i, j + 1)\n","    return results\n","\n","\n","def play_game(agent, human, env, print_board=True):\n","    current_player = None\n","    continue_game = True\n","    while continue_game:\n","        if current_player == agent:\n","            current_player = human\n","        else:\n","            current_player = agent\n","\n","        current_player.take_action(env)\n","\n","        if current_player == agent:\n","            state = env.get_state()\n","            agent.update_state_history(state)\n","            agent.update(env)\n","            if print_board:\n","                env.draw_board()\n","        if env.game_over():\n","            continue_game = False\n","\n","\n","def main(should_learn_before_playing):\n","    print(\"게임시작\")\n","    print(\"에이전트 -> x\")\n","    print(\"플레이어 -> o\")\n","\n","    # initialize empty environment\n","    env = Environment()\n","\n","    state_winner_heptas = get_state_hash_and_winner(env)\n","\n","    # initialize agent as p1\n","    agent = Agent()\n","    agent.set_symbol(env.x)\n","    agent.initialize_V(env, state_winner_heptas)\n","\n","    if should_learn_before_playing:\n","        print(\"학습시작\")\n","        # to learn\n","        agent_to_learn = Agent()\n","        agent_to_learn.set_symbol(env.o)\n","        agent_to_learn.initialize_V(env, state_winner_heptas)\n","\n","        for i in range(10000):\n","            if i > 0 and i % 1000 == 0:\n","                print(f\"에이전트가 {i} 번 게임함\")\n","            play_game(agent, agent_to_learn, Environment(), print_board=False)\n","        print(\"\")\n","        print(\"에이전트가 10,000번의 학습을 마침\")\n","\n","    # play agent vs human\n","    human = Human()\n","    human.set_symbol(env.o)\n","    total_game_played = 0\n","    while True:\n","        env = Environment()\n","        play_game(agent, human, env=env)\n","\n","        if env.winner == env.x:\n","            print(f\"에이전트가 이김\")\n","        elif env.winner == env.o:\n","            print(f\"사람이 이김\")\n","        else:\n","            print(f\"무승부\")\n","\n","        answer = input(\"Play Again? [y/n]: \")\n","        if answer and answer.lower()[0] == 'n':\n","            break"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"TqYCz4zx28wk"},"outputs":[{"name":"stdout","output_type":"stream","text":["게임시작\n","에이전트 -> x\n","플레이어 -> o\n"]},{"ename":"ValueError","evalue":"Maximum allowed dimension exceeded","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main(should_learn_before_playing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","Cell \u001b[1;32mIn[11], line 198\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(should_learn_before_playing)\u001b[0m\n\u001b[0;32m    196\u001b[0m agent \u001b[39m=\u001b[39m Agent()\n\u001b[0;32m    197\u001b[0m agent\u001b[39m.\u001b[39mset_symbol(env\u001b[39m.\u001b[39mx)\n\u001b[1;32m--> 198\u001b[0m agent\u001b[39m.\u001b[39;49minitialize_V(env, state_winner_heptas)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m should_learn_before_playing:\n\u001b[0;32m    201\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m학습시작\u001b[39m\u001b[39m\"\u001b[39m)\n","Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mAgent.initialize_V\u001b[1;34m(self, env, state_winner_triples)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_V\u001b[39m(\u001b[39mself\u001b[39m, env, state_winner_triples):\n\u001b[1;32m----> 8\u001b[0m     V \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(env\u001b[39m.\u001b[39mmax_states)\n\u001b[0;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m state, winner, ended \u001b[39min\u001b[39;00m state_winner_triples:\n\u001b[0;32m     10\u001b[0m         \u001b[39mif\u001b[39;00m ended:\n","\u001b[1;31mValueError\u001b[0m: Maximum allowed dimension exceeded"]}],"source":["  main(should_learn_before_playing=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
